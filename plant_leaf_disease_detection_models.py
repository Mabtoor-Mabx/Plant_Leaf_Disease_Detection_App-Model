# -*- coding: utf-8 -*-
"""Plant_Leaf_Disease_Detection_Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mRA1rfRiqFA6ovo8QLJMM1OesFZyk3SQ

#**Plant Leaf Disease Detection Notebook**

### **(A Project of Horair Ahmad,Mabtoor-Ul-Shafiq and Muhammad Waqas Under the Supervision of Miss Raheela In University of Agriculture Faisalabad)**

Notebook is designed to Classify the category and disease of plants using  Transfer Learning.  **We Used 6 Different Models to get Maximum Accuracy and Minimum Loss.** The 6 Different models are: 

* EfficientNetB0

* MobileNetV2

* EfficientNetV2B3

* Resnet50

* DenseNet121

* InceptionV3



**In our case, EfficientNetB0 model gives us best accuracy. We get 97% accuracy in Feature Extraction and 99% accuracy in Fine-Tuning.**

The Dataset We Using :  https://www.kaggle.com/datasets/arjuntejaswi/plant-village

For More Information: Visit the Github Repositories

https://github.com/horair (Horair Ahmad)

https://github.com/Mabtoor-Mabx (Mabtoor-Ul-Shafiq)

# **1-Checking GPU**
"""

!nvidia-smi -L

"""# **2- Import Libraries**"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# **3- Creating Helper Function**"""

!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py

"""# **4- Import Series of Functions from helping function**"""

# Import series of helper functions for our notebook
from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir

"""# **5- Install and import split folders**"""

pip install split-folders

import splitfolders

"""# **6- Mount Google Drive**"""

from google.colab import drive
drive.mount('/content/drive')

"""# **7- Import Dataset from google Drive**"""

import zipfile

# unzip the downloaded file
zip_ref = zipfile.ZipFile("/content/drive/MyDrive/Deep_Learning_Datasets_For_Practice_Purposes/Plant_leaf_diseases_dataset_with_augmentation.zip")
zip_ref.extractall()
zip_ref.close()

"""# **8-Split Dataset**"""

splitfolders.ratio("/content/Plant_leave_diseases_dataset_with_augmentation", output="output_1",
    seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)

"""# **9- Train , Test and Validation Directories**"""

train_dir_1 = "/content/output_1/train"
val_dir_1 = "/content/output_1/val"
test_dir_1 = "/content/output_1/test"

# How many images/classes are there?
walk_through_dir("output_1")

"""# **10- Setup data inputs**"""

# Set up data inputs
import tensorflow as tf
IMG_SIZE = (224, 224)
train_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1,
                                                                label_mode="categorical",
                                                                 batch_size=32,
                                                                image_size=IMG_SIZE,
                                                                 seed=42,
                                                                 shuffle=True)
val_data =  tf.keras.preprocessing.image_dataset_from_directory(val_dir_1,
                                                                 batch_size=32,                                                                
                                                                 label_mode="categorical",
                                                                 image_size=IMG_SIZE,
                                                                seed=42,
                                                                 shuffle=False) # don't shuffle test data for prediction analysis

test_data =  tf.keras.preprocessing.image_dataset_from_directory(test_dir_1,
                                                                 batch_size=32,                                                                
                                                                 label_mode="categorical",
                                                                 image_size=IMG_SIZE,
                                                                 seed=42,
                                                                 shuffle=False) # don't shuffle test data for prediction analysis

"""# **11- Create Checkpoint Callback**"""

# Create a checkpoint callback
checkpoint_path = "/content/plants"
from keras.callbacks import ModelCheckpoint, EarlyStopping

es = EarlyStopping(monitor="val_accuracy", min_delta=0.01, patience=2, verbose=1)

mc = ModelCheckpoint(monitor="val_accuracy",filepath=checkpoint_path, min_delta=0.01, patience=3, verbose=1, save_best_only=True)

cb = [es, mc]

"""# **12- Data Augmentation**"""

# Create a data augmentation layer to incorporate it right into the model
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

# Setup data augmentation
data_augmentation = Sequential([
   preprocessing.RandomFlip("horizontal"),
   preprocessing.RandomRotation(0.2),
   preprocessing.RandomHeight(0.2),
   preprocessing.RandomWidth(0.2),
   preprocessing.RandomZoom(0.2),
  # preprocessing.Rescaling(1/255.)  # rescale inputs of images between 1 & 0, required for models like ResNet50                            
], name= "data_augmentation")

"""# **Model 1 (EfficientNetB0)**

## **13-Setup Base Model and Freeze the layers Using EfficientNetB0**
"""

# Setup a base model and freeze its layer (this will extract features)
base_model = tf.keras.applications.EfficientNetB0(include_top=False)
base_model.trainable = False

# Setup a model architecture with trainable top layers
inputs = layers.Input(shape=(224, 224, 3), name="input_layer")
x = data_augmentation(inputs) # augment layers (only happens during trainable phase)
x = base_model(x, training=False) # put the base model in interface mode so weights which needs to be frozen, stay frozen
x = layers.GlobalAveragePooling2D(name="global_avg_pooling_layer")(x)
outputs = layers.Dense(len(train_data.class_names), activation="softmax", name="output_layer")(x)
model=tf.keras.Model(inputs, outputs)

# Get a summary of model we've been created
model.summary()

"""## **14-Applying Feature Extraction**"""

from tensorflow.keras import metrics

METRICS = [
      metrics.TruePositives(name='tp'),
      metrics.FalsePositives(name='fp'),
      metrics.TrueNegatives(name='tn'),
      metrics.FalseNegatives(name='fn'), 
      metrics.CategoricalAccuracy(name='accuracy'),
      metrics.Precision(name='precision'),
      metrics.Recall(name='recall'),
      metrics.AUC(name='auc')
]

# Compile
model.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=METRICS)

# Fit
history = model.fit(train_data,
                    epochs=10, # fit to 5 epochs to keep experiment quick
                    validation_data=val_data,
                    validation_steps=len(val_data))

"""## **15-Evaluate Whole Dataset**"""

# Evaluate on the whole test dataset  ##its a feature extraction instead of fine tune
feature_extraction_results = model.evaluate(test_data)
feature_extraction_results

"""## **16-Graph of Feature Extraction**"""

plot_loss_curves(history)

"""## **17-Fine-Tuning Model Using EfficientNetB0**"""

# Unfreeze all of the layers in base model
base_model.trainable=True

# Refreeze every layer except the last 5 layer 
for layer in base_model.layers[:-5]:
  layer.trainable=False

# Recompile model with lower learning rate (it's typically best practice to lower the learning rate when fine-tuning)
model.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # learning rate lower by 10x
              metrics=METRICS)

# What layers in the model are trainable?
for layer in model.layers:
  print(layer.name, layer.trainable)

# Check which layers in our model is trainable
for layer_number, layer in enumerate(model.layers[2].layers):
  print(layer_number, layer.name, layer.trainable )

# Fine-tune for more 5 epochs
fine_tune_epochs = 20 # model has already done the 5 epochs (feature extraction), this is the total number of epochs we're after (5 + 5 =10)

# Fine-tune our model
history_fine_tune = model.fit(train_data,
                              epochs=fine_tune_epochs,
                              validation_data=val_data,
                              validation_steps=len(val_data),
                              initial_epoch=history.epoch[-1])

"""## **18- Evaluate Fine-Tune Model**"""

# Evaluate on the whole test dataset
fine_tune_results =model.evaluate(test_data)
fine_tune_results

"""## **19-Compare The Histories**"""

# Compare the historys of feature extraction model with fine-tuning model 
compare_historys(original_history=history,
                 new_history=history_fine_tune,
                 initial_epochs=5)

"""## **20-Saving and Loading The Model**"""

# # Save our fine-tuning model
# model.save("drive/MyDrive/Deep_Learning_Model/Plant_leaf_disease_Detection_Models")

# # Load and evaluate saved model
# loaded_model =tf.keras.models.load_model("drive/MyDrive/Deep_Learning_Model/Plant_leaf_disease_Detection_Models")

# # Evaluate loaded model and compare performance to pre-saved model
# loaded_model_results = loaded_model.evaluate(test_data)
# loaded_model_results

# The results from the saved model (Above) should be very similar to the results below
fine_tune_results

"""## **21-Making Prediction with Trained Model**"""

# Make predictions with model
preds_probs = model.predict(test_data, verbose=1) # set verbosity to see how long it left

len(test_data)

# How many predictions are there?
len(preds_probs)

# What's the shape of our predictions?
preds_probs.shape

# Let's see whats the first 10 predictions looks like
preds_probs[:10]

# What does the first prediction probability array look like?
preds_probs[0], len(preds_probs[0]), sum(preds_probs[0])

# We get one prediction probability per class(in our case there's 101 prediction probabilities)
print(f"Number of prediction probabilites for sample 0: {len(preds_probs[0])}")
print(f"What prediction probabilites sample 0 looks like:\n {preds_probs[0]}")
print(f"The class with highest predicted probability by the model for sample 0: {preds_probs[0].argmax()}")

# Get the pred classes of each model
pred_classes = preds_probs.argmax(axis=1)

# How do they look like?
pred_classes[:10]

# How many pred classes we have?
len(pred_classes)

# To get our test dataset labels we need to unravel our test_data BatchDataset
y_labels = []
for images, labels in test_data.unbatch():
  y_labels.append(labels.numpy().argmax()) # currently test labels look like: [0, 0, 0, 1,.....0,0], we want the index value where the index value "1" occurs
y_labels[:10] # look at the first 10

# How many y_labels are there?
len(y_labels)

test_data

len(test_data)

"""## **22-Lets Create Confusion Matrix**"""

# The results from the saved model (Above) should be very similar to the results below
fine_tune_results

# Get a list of class names
class_names =test_data.class_names
class_names[:10]

import itertools
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix

# We need to make some changes to our make_confusion_matrix function to ensure the x-label print verticaly
def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): 
  """Makes a labelled confusion matrix comparing predictions and ground truth labels.

  If classes is passed, confusion matrix will be labelled, if not, integer class values
  will be used.

  Args:
    y_true: Array of truth labels (must be same shape as y_pred).
    y_pred: Array of predicted labels (must be same shape as y_true).
    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.
    figsize: Size of output figure (default=(10, 10)).
    text_size: Size of output figure text (default=15).
    norm: normalize values or not (default=False).
    savefig: save confusion matrix to file (default=False).
  
  Returns:
    A labelled confusion matrix plot comparing y_true and y_pred.

  Example usage:
    make_confusion_matrix(y_true=test_labels, # ground truth test labels
                          y_pred=y_preds, # predicted labels
                          classes=class_names, # array of class label names
                          figsize=(15, 15),
                          text_size=10)
  """  
  # Create the confustion matrix
  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
  n_classes = cm.shape[0] # find the number of classes we're dealing with

  # Plot the figure and make it pretty
  fig, ax = plt.subplots(figsize=figsize)
  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better
  fig.colorbar(cax)

  # Are there a list of classes?
  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])
  
  # Label the axes
  ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes), # create enough axis slots for each class
         yticks=np.arange(n_classes), 
         xticklabels=labels, # axes will labeled with class names (if they exist) or ints
         yticklabels=labels)
  
  # Make x-axis labels appear on bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  ### changes (x-labels vertically) ###
  plt.xticks(rotation=70, fontsize=text_size)
  plt.yticks(fontsize=text_size)

  # Set the threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    if norm:
      plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)
    else:
      plt.text(j, i, f"{cm[i, j]}",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)

  # Save the figure to the current working directory
  if savefig:
    fig.savefig("confusion_matrix.png")

make_confusion_matrix(y_true=y_labels,
                       y_pred=pred_classes,
                       classes=class_names,
                      figsize=(100, 100),
                      text_size=20,
                      savefig=True)

"""## **23-To Keep Evaluation On Track, Let's Create Classification Report**"""

from sklearn.metrics import classification_report
print(classification_report(y_true=y_labels,
                            y_pred=pred_classes))

# Get a dictionary of the classification report
classification_report_dict = classification_report(y_labels, pred_classes, output_dict=True)
classification_report_dict

class_names[37]

classification_report_dict["37"]["f1-score"]

# Create empty dictionary
class_f1_scores = {}
# Loop through classification report dictionary items
for k, v in classification_report_dict.items():
  if k == "accuracy": # stop once we get to accuracy key
    break
  else:
     # Add names and f1-scores to new dictionary
     class_f1_scores[class_names[int(k)]] = v["f1-score"]
class_f1_scores

# Turn f1 scores into Dataframe visualization
import pandas as pd
f1_scores =pd.DataFrame({"class_names": list(class_f1_scores.keys()),
                        "f1-score": list(class_f1_scores.values())}).sort_values("f1-score", ascending=False)
f1_scores

f1_scores[:10]

import matplotlib.pyplot as plt


import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12, 25))
scores = ax.barh(range(len(f1_scores)), f1_scores["f1-score"].values) # get f1-score value
ax.set_yticks(range(len(f1_scores)))
ax.set_yticklabels(f1_scores["class_names"])
ax.set_xlabel("F1-score")
ax.set_title("F1 score for 101 Different Food Classes (predicted by food vision mini)")
ax.invert_yaxis(); # reverse the order of our plot


# Challenge: add value to the end of each bar of what the actual f1-score is 
# (hint: use the "autolabel" function from here: https://matplotlib.org/2.0.2/examples/api/barchart_demo.html)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open("model.tflite", "wb") as f:
  f.write(tflite_model)

"""# **Model 2 (MobileNetV2)**

## **24- Data Augmentation**
"""

# Create a data augmentation layer to incorporate it right into the model
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

# Setup data augmentation
data_augmentation_2 = Sequential([
   preprocessing.RandomFlip("horizontal"),
   preprocessing.RandomRotation(0.2),
   preprocessing.Rescaling(1/255.)  # rescale inputs of images between 1 & 0, required for models like ResNet50                            
], name= "data_augmentation")

"""## **25-Compile and Evaluate The Model**"""

# Setup a base model and freeze its layer (this will extract features)
base_model = tf.keras.applications.MobileNetV2(include_top=False)
base_model.trainable = False

# Setup a model architecture with trainable top layers
inputs = layers.Input(shape=(224, 224, 3), name="input_layer")
x = data_augmentation_2(inputs) # augment layers (only happens during trainable phase)
x = base_model(x, training=False) # put the base model in interface mode so weights which needs to be frozen, stay frozen
x = layers.GlobalAveragePooling2D(name="global_avg_pooling_layer")(x)
outputs = layers.Dense(len(train_data.class_names), activation="softmax", name="output_layer")(x)
model_mobileNet=tf.keras.Model(inputs, outputs)

model_mobileNet.summary()

# Compile
model_mobileNet.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=METRICS)

# Fit
history_mobileNet_fex = model_mobileNet.fit(train_data,
                                            epochs=10, # fit to 5 epochs to keep experiment quick
                                            validation_data=val_data,
                                            validation_steps=len(val_data))

result_mobileNet_fex = model_mobileNet.evaluate(test_data)
result_mobileNet_fex

"""## **26-Graph of Feature Extraction**"""

plot_loss_curves(history_mobileNet_fex)

"""## **27- Finetuning With MobileNetV2**"""

# Unfreeze all of the layers in base model
base_model.trainable=True

# Refreeze every layer except the last 5 layer 
for layer in base_model.layers[:-5]:
  layer.trainable=False

# Recompile model with lower learning rate (it's typically best practice to lower the learning rate when fine-tuning)
model_mobileNet.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # learning rate lower by 10x
              metrics=METRICS)

# What layers in the model are trainable?
for layer in model_mobileNet.layers:
  print(layer.name, layer.trainable)

for layer_number, layer in enumerate(model_mobileNet.layers[2].layers):
  print(layer_number, layer.name, layer.trainable)

"""## **28-Recompile and Evaluate The Model**"""

fine_tune_epochs = 20 # model has already done the 5 epochs (feature extraction), this is the total number of epochs we're after (5 + 5 =10)

# Fine-tune our model
history_mobileNet_fine_tune = model_mobileNet.fit(train_data,
                                            epochs=20, # fit to 5 epochs to keep experiment quick
                                            validation_data=test_data,
                                            validation_steps=int(0.15 * len(test_data)),
                                            initial_epoch=history_mobileNet_fex.epoch[-1])

result_fine_tune_mobileNet = model_mobileNet.evaluate(test_data)
result_fine_tune_mobileNet

"""## **29-Compare Both Histories**"""

compare_historys(original_history=history_mobileNet_fex,
                 new_history=history_mobileNet_fine_tune,
                 initial_epochs=5)

"""## **29-Saving MobileNetV2 Model**"""

# # Save our fine-tuning model
# model_mobileNet.save("drive/MyDrive/Project/Mobile_NetV2Model")

"""## **30-Evaluating All Accross**"""

# Make predictions with model
preds_probs_mobileNet = model_mobileNet.predict(test_data, verbose=1) # set verbosity to see how long it left

# We get one prediction probability per class(in our case there's 101 prediction probabilities)
print(f"Number of prediction probabilites for sample 0: {len(preds_probs_mobileNet[0])}")
print(f"What prediction probabilites sample 0 looks like:\n {preds_probs_mobileNet[0]}")
print(f"The class with highest predicted probability by the model for sample 0: {preds_probs_mobileNet[0].argmax()}")

# Get the pred classes of each model
pred_classes_mobileNet = preds_probs_mobileNet.argmax(axis=1)

# How do they look like?
pred_classes_mobileNet[:10]

# To get our test dataset labels we need to unravel our test_data BatchDataset
y_labels_mobileNet = []
for images, labels in test_data.unbatch():
  y_labels_mobileNet.append(labels.numpy().argmax()) # currently test labels look like: [0, 0, 0, 1,.....0,0], we want the index value where the index value "1" occurs
y_labels_mobileNet[:10] # look at the first 10

# The results from the saved model (Above) should be very similar to the results below
result_fine_tune_mobileNet

# Get a list of class names
class_names =test_data.class_names
class_names[:10]

make_confusion_matrix(y_true=y_labels_mobileNet,
                       y_pred=pred_classes_mobileNet,
                       classes=class_names,
                      figsize=(100, 100),
                      text_size=20,
                      savefig=True)

"""## **31-Classification Report of MobileNetV2**"""

from sklearn.metrics import classification_report
print(classification_report(y_true=y_labels_mobileNet,
                            y_pred=pred_classes_mobileNet))

# Get a dictionary of the classification report
classification_report_dict_mobileNet = classification_report(y_labels_mobileNet, pred_classes_mobileNet, output_dict=True)
classification_report_dict_mobileNet

# Create empty dictionary
class_f1_scores = {}
# Loop through classification report dictionary items
for k, v in classification_report_dict_mobileNet.items():
  if k == "accuracy": # stop once we get to accuracy key
    break
  else:
     # Add names and f1-scores to new dictionary
     class_f1_scores[class_names[int(k)]] = v["f1-score"]
class_f1_scores

# Turn f1 scores into Dataframe visualization
import pandas as pd
f1_scores =pd.DataFrame({"class_names": list(class_f1_scores.keys()),
                        "f1-score": list(class_f1_scores.values())}).sort_values("f1-score", ascending=False)
f1_scores

import matplotlib.pyplot as plt


import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12, 25))
scores = ax.barh(range(len(f1_scores)), f1_scores["f1-score"].values) # get f1-score value
ax.set_yticks(range(len(f1_scores)))
ax.set_yticklabels(f1_scores["class_names"])
ax.set_xlabel("F1-score")
ax.set_title("F1 score for 101 Different Food Classes (predicted by food vision mini)")
ax.invert_yaxis(); # reverse the order of our plot

"""# **Model 3(EfficientNetV2B3)**

## **32-Data Augmentation**
"""

# Create a data augmentation layer to incorporate it right into the model
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

# Setup data augmentation
data_augmentation_3 = Sequential([
   preprocessing.RandomFlip("horizontal"),
   preprocessing.RandomRotation(0.2),
   preprocessing.Rescaling(1/255.)  # rescale inputs of images between 1 & 0, required for models like ResNet50                            
], name= "data_augmentation")

# Setup a base model and freeze its layer (this will extract features)
base_model = tf.keras.applications.EfficientNetV2B3(include_top=False)
base_model.trainable = False

# Setup a model architecture with trainable top layers
inputs = layers.Input(shape=(224, 224, 3), name="input_layer")
#x = data_augmentation(inputs) # augment layers (only happens during trainable phase)
x = base_model(inputs, training=False) # put the base model in interface mode so weights which needs to be frozen, stay frozen
x = layers.GlobalAveragePooling2D(name="global_avg_pooling_layer")(x)
x = layers.Dense(1024, activation="sigmoid")(x)
x = layers.Dense(1024, activation="sigmoid")(x)
x = layers.Dense(1024, activation="sigmoid")(x)
outputs = layers.Dense(len(train_data.class_names), activation="softmax", name="output_layer")(x)
model_efficientNetB3 = tf.keras.Model(inputs, outputs)

model_efficientNetB3.summary()

"""## **33-Compile and Evaluate The Model**"""

# Compile
model_efficientNetB3.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=METRICS)

# Fit
history_efficientNetb3_fex = model_efficientNetB3.fit(train_data,
                                            epochs=10, # fit to 5 epochs to keep experiment quick
                                            validation_data=val_data,
                                            validation_steps=int(0.25 * len(val_data)))

result_effientNetb3_fex = model_efficientNetB3.evaluate(test_data)
result_effientNetb3_fex

"""## **34-Plot for EfficientNetV2B3**"""

plot_loss_curves(history_efficientNetb3_fex)

"""## **35- Fine Tuning With EfficientNetV2B3**"""

# Unfreeze all of the layers in base model
base_model.trainable=True

# Refreeze every layer except the last 5 layer 
for layer in base_model.layers[:-5]:
  layer.trainable=False

"""## **36-Recompile and Evaluate the Model**"""

# Recompile model with lower learning rate (it's typically best practice to lower the learning rate when fine-tuning)
model_efficientNetB3.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # learning rate lower by 10x
              metrics=METRICS)

# What layers in the model are trainable?
for layer in model_efficientNetB3.layers:
  print(layer.name, layer.trainable)

for layer_number, layer in enumerate(model_efficientNetB3.layers[1].layers):
  print(layer_number, layer.name, layer.trainable)

fine_tune_epochs = 20 # model has already done the 5 epochs (feature extraction), this is the total number of epochs we're after (5 + 5 =10)

# Fine-tune our model
history_effNetv2b3_fine_tune = model_efficientNetB3.fit(train_data,
                                                        epochs=fine_tune_epochs, # fit to 5 epochs to keep experiment quick
                                                        validation_data=val_data,
                                                        validation_steps=int(0.25 * len(val_data)),
                                                        initial_epoch=history_efficientNetb3_fex.epoch[-1])

result_fine_tune_efficientNetv2b3 = model_efficientNetB3.evaluate(test_data)
result_fine_tune_efficientNetv2b3

"""## **37-Compare Histories**"""

compare_historys(original_history=history_efficientNetb3_fex,
                 new_history=history_effNetv2b3_fine_tune,
                 initial_epochs=5)

"""## **38-Save and Load the Model**"""

# model_efficientNetB3.save("drive/MyDrive/tensorflow_course/Project/EfficientNetV2B3Model")

# model_efficientNetB3.save("my_model.h5")

# from keras.models import load_model

# loaded_model = load_model('my_model.h5')

# loaded_model.evaluate(test_data)

"""## **39-Make Predictions With Model**"""

# Make predictions with model
preds_probs_efcNetV2 = model_efficientNetB3.predict(test_data, verbose=1) # set verbosity to see how long it left

# We get one prediction probability per class(in our case there's 101 prediction probabilities)
print(f"Number of prediction probabilites for sample 0: {len(preds_probs_efcNetV2[0])}")
print(f"What prediction probabilites sample 0 looks like:\n {preds_probs_efcNetV2[0]}")
print(f"The class with highest predicted probability by the model for sample 0: {preds_probs_efcNetV2[0].argmax()}")

# Get the pred classes of each model
pred_classes_efcNetV2 = preds_probs_efcNetV2.argmax(axis=1)

# How do they look like?
pred_classes_efcNetV2[:10]

# To get our test dataset labels we need to unravel our test_data BatchDataset
y_labels_efcNet = []
for images, labels in test_data.unbatch():
  y_labels_efcNet.append(labels.numpy().argmax()) # currently test labels look like: [0, 0, 0, 1,.....0,0], we want the index value where the index value "1" occurs
y_labels_efcNet[:10] # look at the first 10

"""## **40-Create Confusion Matrix**"""

make_confusion_matrix(y_true=y_labels_efcNet,
                       y_pred=pred_classes_efcNetV2,
                       classes=class_names,
                      figsize=(100, 100),
                      text_size=20,
                      savefig=True)

"""# **Model 4 (RESNET50)**

## **41-Data Augmentation**
"""

# Create a data augmentation layer to incorporate it right into the model
from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

# Setup data augmentation
data_augmentation_4 = Sequential([
   preprocessing.RandomFlip("horizontal"),
   preprocessing.RandomRotation(0.2),
   preprocessing.RandomHeight(0.2),
   preprocessing.RandomWidth(0.2),
   preprocessing.RandomZoom(0.2),
   preprocessing.Rescaling(1/255.)  # rescale inputs of images between 1 & 0, required for models like ResNet50                            
], name= "data_augmentation")

"""## **42-Compile and Evaluate RESNET50 Model**"""

# Setup a base model and freeze its layer (this will extract features)
base_model_4 = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')
base_model_4.trainable = False

# Setup a model architecture with trainable top layers
inputs = layers.Input(shape=(224, 224, 3), name="input_layer")
x = data_augmentation_4(inputs) # augment layers (only happens during trainable phase)
x = base_model_4(x, training=False) # put the base model in interface mode so weights which needs to be frozen, stay frozen
x = layers.GlobalAveragePooling2D(name="global_avg_pooling_layer")(x)
outputs = layers.Dense(len(train_data.class_names), activation="softmax", name="output_layer")(x)
model_4=tf.keras.Model(inputs, outputs)

# Get a summary of model we've been created
model_4.summary()

from tensorflow.keras import metrics

METRICS = [
      metrics.TruePositives(name='tp'),
      metrics.FalsePositives(name='fp'),
      metrics.TrueNegatives(name='tn'),
      metrics.FalseNegatives(name='fn'), 
      metrics.CategoricalAccuracy(name='accuracy'),
      metrics.Precision(name='precision'),
      metrics.Recall(name='recall'),
      metrics.AUC(name='auc')
]

# Compile
model_4.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=METRICS)

# Fit
history_4 = model.fit(train_data,
                    epochs=10, # fit to 5 epochs to keep experiment quick
                    validation_data=val_data,
                    validation_steps=len(val_data))

# Evaluate on the whole test dataset  ##its a feature extraction instead of fine tune
feature_extraction_results = model_4.evaluate(test_data)
feature_extraction_results

"""## **43- Plot the Graph for RESNET50**"""

plot_loss_curves(history_4)

"""## **44- Fine-Tune Model With RESNET50**"""

# Unfreeze all of the layers in base model
base_model.trainable=True

# Refreeze every layer except the last 5 layer 
for layer in base_model.layers[:-5]:
  layer.trainable=False

# Recompile model with lower learning rate (it's typically best practice to lower the learning rate when fine-tuning)
model_4.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # learning rate lower by 10x
              metrics=METRICS)

# What layers in the model are trainable?
for layer in model.layers:
  print(layer.name, layer.trainable)

# Check which layers in our model is trainable
for layer_number, layer in enumerate(model.layers[2].layers):
  print(layer_number, layer.name, layer.trainable )

"""## **45-Recompile and Evaluate The Model**"""

# Fine-tune for more 5 epochs
fine_tune_epochs_4 = 20 # model has already done the 5 epochs (feature extraction), this is the total number of epochs we're after (5 + 5 =10)

# Fine-tune our model
history_fine_tune_4 = model.fit(train_data,
                              epochs=fine_tune_epochs_4,
                              validation_data=val_data,
                              validation_steps=len(val_data),
                              initial_epoch=history_4.epoch[-1])

# Evaluate on the whole test dataset
fine_tune_results =model_4.evaluate(test_data)
fine_tune_results

"""## **46-Compare The Histories**"""

# Compare the historys of feature extraction model with fine-tuning model 
compare_historys(original_history=history_4,
                 new_history=history_fine_tune_4,
                 initial_epochs=5)

"""## **47-Make Predictions with RESNET50 Model**"""

# Make predictions with model
preds_probs_4 = model_4.predict(test_data, verbose=1) # set verbosity to see how long it left

len(test_data)

# How many predictions are there?
len(preds_probs_4)

# What's the shape of our predictions?
preds_probs_4.shape

# Let's see whats the first 10 predictions looks like
preds_probs_4[:10]

# What does the first prediction probability array look like?
preds_probs_4[0], len(preds_probs_4[0]), sum(preds_probs_4[0])

# We get one prediction probability per class(in our case there's 101 prediction probabilities)
print(f"Number of prediction probabilites for sample 0: {len(preds_probs_4[0])}")
print(f"What prediction probabilites sample 0 looks like:\n {preds_probs_4[0]}")
print(f"The class with highest predicted probability by the model for sample 0: {preds_probs_4[0].argmax()}")

# Get the pred classes of each model
pred_classes_4 = preds_probs_4.argmax(axis=1)

# How do they look like?
pred_classes_4[:10]

# How many pred classes we have?
len(pred_classes_4)

# To get our test dataset labels we need to unravel our test_data BatchDataset
y_labels = []
for images, labels in test_data.unbatch():
  y_labels.append(labels.numpy().argmax()) # currently test labels look like: [0, 0, 0, 1,.....0,0], we want the index value where the index value "1" occurs
y_labels[:10] # look at the first 10

# How many y_labels are there?
len(y_labels)

# The results from the saved model (Above) should be very similar to the results below
fine_tune_results

# Get a list of class names
class_names =test_data.class_names
class_names[:10]

"""## **48-Make Confusion Matrix For RESNET50**"""

import itertools
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix

# We need to make some changes to our make_confusion_matrix function to ensure the x-label print verticaly
def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): 
  """Makes a labelled confusion matrix comparing predictions and ground truth labels.

  If classes is passed, confusion matrix will be labelled, if not, integer class values
  will be used.

  Args:
    y_true: Array of truth labels (must be same shape as y_pred).
    y_pred: Array of predicted labels (must be same shape as y_true).
    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.
    figsize: Size of output figure (default=(10, 10)).
    text_size: Size of output figure text (default=15).
    norm: normalize values or not (default=False).
    savefig: save confusion matrix to file (default=False).
  
  Returns:
    A labelled confusion matrix plot comparing y_true and y_pred.

  Example usage:
    make_confusion_matrix(y_true=test_labels, # ground truth test labels
                          y_pred=y_preds, # predicted labels
                          classes=class_names, # array of class label names
                          figsize=(15, 15),
                          text_size=10)
  """  
  # Create the confustion matrix
  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
  n_classes = cm.shape[0] # find the number of classes we're dealing with

  # Plot the figure and make it pretty
  fig, ax = plt.subplots(figsize=figsize)
  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better
  fig.colorbar(cax)

  # Are there a list of classes?
  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])
  
  # Label the axes
  ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes), # create enough axis slots for each class
         yticks=np.arange(n_classes), 
         xticklabels=labels, # axes will labeled with class names (if they exist) or ints
         yticklabels=labels)
  
  # Make x-axis labels appear on bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  ### changes (x-labels vertically) ###
  plt.xticks(rotation=70, fontsize=text_size)
  plt.yticks(fontsize=text_size)

  # Set the threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    if norm:
      plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)
    else:
      plt.text(j, i, f"{cm[i, j]}",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)

  # Save the figure to the current working directory
  if savefig:
    fig.savefig("confusion_matrix.png")

make_confusion_matrix(y_true=y_labels,
                       y_pred=pred_classes_4,
                       classes=class_names,
                      figsize=(100, 100),
                      text_size=20,
                      savefig=True)

"""# **Model 5 (DenseNET121)**

## **49- Compile and Evaluate the Model**
"""

# Setup a base model and freeze its layer (this will extract features)
base_model_5 = tf.keras.applications.DenseNet121(include_top=False, weights='imagenet')
base_model_5.trainable = False

# Setup a model architecture with trainable top layers
inputs = layers.Input(shape=(224, 224, 3), name="input_layer")
x = data_augmentation(inputs) # augment layers (only happens during trainable phase)
x = base_model_5(x, training=False) # put the base model in interface mode so weights which needs to be frozen, stay frozen
x = layers.GlobalAveragePooling2D(name="global_avg_pooling_layer")(x)
outputs = layers.Dense(len(train_data.class_names), activation="softmax", name="output_layer")(x)
model_5=tf.keras.Model(inputs, outputs)

# Get a summary of model we've been created
model_5.summary()

# Compile
model_5.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=METRICS)

# Fit
history_5 = model_5.fit(train_data,
                    epochs=10, # fit to 5 epochs to keep experiment quick
                    validation_data=val_data,
                    validation_steps=len(val_data))

# Evaluate on the whole test dataset  ##its a feature extraction instead of fine tune
feature_extraction_denseNet_results = model_5.evaluate(test_data)
feature_extraction_denseNet_results

"""## **50-Plot Graph for DenseNet121**"""

plot_loss_curves(history_5)

"""## **51-Fine Tune DenseNet121 Model**"""

# Unfreeze all of the layers in base model
base_model_5.trainable=True

# Refreeze every layer except the last 5 layer 
for layer in base_model_5.layers[:-5]:
  layer.trainable=False

# Recompile model with lower learning rate (it's typically best practice to lower the learning rate when fine-tuning)
model_5.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # learning rate lower by 10x
              metrics=METRICS)

# What layers in the model are trainable?
for layer in model_5.layers:
  print(layer.name, layer.trainable)

# Check which layers in our model is trainable
for layer_number, layer in enumerate(model_5.layers[2].layers):
  print(layer_number, layer.name, layer.trainable )

"""## **52-Recompile and Evaluate The Model**"""

# Fine-tune for more 5 epochs
fine_tune_epochs_5 = 20 # model has already done the 5 epochs (feature extraction), this is the total number of epochs we're after (5 + 5 =10)

# Fine-tune our model
history_fine_tune_5 = model_5.fit(train_data,
                              epochs=fine_tune_epochs_5,
                              validation_data=val_data,
                              validation_steps=len(val_data),
                              initial_epoch=history_5.epoch[-1])

# Evaluate on the whole test dataset
fine_tune_dense_results =model_5.evaluate(test_data)
fine_tune_dense_results

"""## **53-Compare The Histories**"""

# Compare the historys of feature extraction model with fine-tuning model 
compare_historys(original_history=history_5,
                 new_history=history_fine_tune_5,
                 initial_epochs=10)

"""## **54-Make Predictions**"""

# Make predictions with model
preds_probs_5 = model_5.predict(test_data, verbose=1) # set verbosity to see how long it left

# What does the first prediction probability array look like?
preds_probs_5[0], len(preds_probs_5[0]), sum(preds_probs_5[0])

# What does the first prediction probability array look like?
preds_probs_5[0], len(preds_probs_5[0]), sum(preds_probs_5[0])

# We get one prediction probability per class(in our case there's 101 prediction probabilities)
print(f"Number of prediction probabilites for sample 0: {len(preds_probs_5[0])}")
print(f"What prediction probabilites sample 0 looks like:\n {preds_probs_5[0]}")
print(f"The class with highest predicted probability by the model for sample 0: {preds_probs_5[0].argmax()}")

# Get the pred classes of each model
pred_classes_5 = preds_probs_5.argmax(axis=1)

# How do they look like?
pred_classes_5[:10]

# To get our test dataset labels we need to unravel our test_data BatchDataset
y_labels = []
for images, labels in test_data.unbatch():
  y_labels.append(labels.numpy().argmax()) # currently test labels look like: [0, 0, 0, 1,.....0,0], we want the index value where the index value "1" occurs
y_labels[:10] # look at the first 10

# Get a list of class names
class_names =test_data.class_names
class_names[:10]

"""## **55-Make Confusion Matrix**"""

import itertools
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix

# We need to make some changes to our make_confusion_matrix function to ensure the x-label print verticaly
def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): 
  """Makes a labelled confusion matrix comparing predictions and ground truth labels.

  If classes is passed, confusion matrix will be labelled, if not, integer class values
  will be used.

  Args:
    y_true: Array of truth labels (must be same shape as y_pred).
    y_pred: Array of predicted labels (must be same shape as y_true).
    classes: Array of class labels (e.g. string form). If `None`, integer labels are used.
    figsize: Size of output figure (default=(10, 10)).
    text_size: Size of output figure text (default=15).
    norm: normalize values or not (default=False).
    savefig: save confusion matrix to file (default=False).
  
  Returns:
    A labelled confusion matrix plot comparing y_true and y_pred.

  Example usage:
    make_confusion_matrix(y_true=test_labels, # ground truth test labels
                          y_pred=y_preds, # predicted labels
                          classes=class_names, # array of class label names
                          figsize=(15, 15),
                          text_size=10)
  """  
  # Create the confustion matrix
  cm = confusion_matrix(y_true, y_pred)
  cm_norm = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis] # normalize it
  n_classes = cm.shape[0] # find the number of classes we're dealing with

  # Plot the figure and make it pretty
  fig, ax = plt.subplots(figsize=figsize)
  cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how 'correct' a class is, darker == better
  fig.colorbar(cax)

  # Are there a list of classes?
  if classes:
    labels = classes
  else:
    labels = np.arange(cm.shape[0])
  
  # Label the axes
  ax.set(title="Confusion Matrix",
         xlabel="Predicted label",
         ylabel="True label",
         xticks=np.arange(n_classes), # create enough axis slots for each class
         yticks=np.arange(n_classes), 
         xticklabels=labels, # axes will labeled with class names (if they exist) or ints
         yticklabels=labels)
  
  # Make x-axis labels appear on bottom
  ax.xaxis.set_label_position("bottom")
  ax.xaxis.tick_bottom()

  ### changes (x-labels vertically) ###
  plt.xticks(rotation=70, fontsize=text_size)
  plt.yticks(fontsize=text_size)

  # Set the threshold for different colors
  threshold = (cm.max() + cm.min()) / 2.

  # Plot the text on each cell
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    if norm:
      plt.text(j, i, f"{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)
    else:
      plt.text(j, i, f"{cm[i, j]}",
              horizontalalignment="center",
              color="white" if cm[i, j] > threshold else "black",
              size=text_size)

  # Save the figure to the current working directory
  if savefig:
    fig.savefig("confusion_matrix.png")

make_confusion_matrix(y_true=y_labels,
                       y_pred=pred_classes_5,
                       classes=class_names,
                      figsize=(100, 100),
                      text_size=20,
                      savefig=True)

"""# **Model 6 (Inception V3)**

## **56-Compile and Evaluate The Model**
"""

# Setup a base model and freeze its layer (this will extract features)
base_model_6 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')
base_model_6.trainable = False

# Setup a model architecture with trainable top layers
inputs = layers.Input(shape=(224, 224, 3), name="input_layer")
x = data_augmentation(inputs) # augment layers (only happens during trainable phase)
x = base_model_6(x, training=False) # put the base model in interface mode so weights which needs to be frozen, stay frozen
x = layers.GlobalAveragePooling2D(name="global_avg_pooling_layer")(x)
x = layers.Dense(1024, activation='relu')(x)
outputs = layers.Dense(len(train_data.class_names), activation="softmax", name="output_layer")(x)
model_6=tf.keras.Model(inputs, outputs)

# Get a summary of model we've been created
model_6.summary()

# Compile
model_6.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(),
              metrics=METRICS)

# Fit
history_6 = model_6.fit(train_data,
                    epochs=10, # fit to 10 epochs to keep experiment quick
                    validation_data=val_data,
                    validation_steps=len(val_data))

inception_feature_extractor = model_6.evaluate(test_data)
inception_feature_extractor

"""## **57-Fine-Tune Inception V3 Model**"""

# Unfreeze all of the layers in base model
base_model_6.trainable=True

# Refreeze every layer except the last 5 layer 
for layer in base_model_6.layers[:-5]:
  layer.trainable=False

# Recompile model with lower learning rate (it's typically best practice to lower the learning rate when fine-tuning)
model_6.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # learning rate lower by 10x
              metrics=METRICS)

"""## **58-Recompile and Evaluate The Model**"""

# Fine-tune for more 5 epochs
fine_tune_epochs_6 = 20 # model has already done the 5 epochs (feature extraction), this is the total number of epochs we're after (5 + 5 =10)

# Fine-tune our model
history_fine_tune_6 = model_6.fit(train_data,
                              epochs=fine_tune_epochs_6,
                              validation_data=val_data,
                              validation_steps=len(val_data),
                              initial_epoch=history_6.epoch[-1])

# Evaluate on the whole test dataset
fine_tune_inception_results =model_6.evaluate(test_data)
fine_tune_inception_results

"""## **59-Compare Histories**"""

# Compare the historys of feature extraction model with fine-tuning model 
compare_historys(original_history=history_6,
                 new_history=history_fine_tune_6,
                 initial_epochs=10)

"""## **60-Make Predictions**"""

# Make predictions with model
preds_probs_6 = model_6.predict(test_data, verbose=1) # set verbosity to see how long it left

# We get one prediction probability per class(in our case there's 101 prediction probabilities)
print(f"Number of prediction probabilites for sample 0: {len(preds_probs_6[0])}")
print(f"What prediction probabilites sample 0 looks like:\n {preds_probs_6[0]}")
print(f"The class with highest predicted probability by the model for sample 0: {preds_probs_6[0].argmax()}")

# Get the pred classes of each model
pred_classes_6 = preds_probs_6.argmax(axis=1)

# How do they look like?
pred_classes_6[:10]

# To get our test dataset labels we need to unravel our test_data BatchDataset
y_labels = []
for images, labels in test_data.unbatch():
  y_labels.append(labels.numpy().argmax()) # currently test labels look like: [0, 0, 0, 1,.....0,0], we want the index value where the index value "1" occurs
y_labels[:10] # look at the first 10

# Get a list of class names
class_names =test_data.class_names
class_names[:10]

"""## **61-Make Confusion Matrix**"""

make_confusion_matrix(y_true=y_labels,
                       y_pred=pred_classes_6,
                       classes=class_names,
                      figsize=(100, 100),
                      text_size=20,
                      savefig=True)